#!/usr/bin/env python
#
# Scrape the FEC site for all Press Releases.
# Writes each PR to a unique .json file with meta+content.
#
# Usage:
#  python fec-press-release-scraper

from lxml import html
from lxml.etree import XPath
from lxml.etree import tostring
import hashlib
import requests
import pprint
import json
import os
import sys
import re

base_url = 'http://www.fec.gov/press/'
release_list = 'news_releases.shtml'

###############################################################
# helper methods
def fetch_press_release(href):
  page = requests.get(base_url + href)
  tree = html.fromstring(page.content)
  #print(page.content)
  pr_els = tree.xpath('//div[@class="press_release_content"]')

  # TODO smarter pattern recognition, maybe based on <h*>News Releases ??
  #if not pr_els:
    #pr_els = tree.xpath('//table')

  return pr_els


def parse_press_release(pr):
  # remove all the 'style' attributes
  for tag in pr.xpath('//*[@style]'):
    tag.attrib.pop('style')

  pr_data = { 'html': tostring(pr).decode('UTF-8') }

  return pr_data


def debug_html_element(el):
  print('HTML: %s' % tostring(el, pretty_print=True).decode('UTF-8'))


###############################################################
# main

# fetch the master list, or use local cached version
if not os.path.isfile(release_list):
  page = requests.get(base_url + release_list)
  with open(release_list, 'w') as cached_release_list:
    cached_release_list.write(page.content.decode('latin1'))

# create output dir if it does not yet exist
if not os.path.isdir('./json'):
  os.mkdir('./json')

# parse the master list
filehandle = open(release_list, 'r')
tree = html.fromstring(filehandle.read())

news_releases = tree.xpath('//table[@id="news_releases"]/tbody/tr')
cell_text = XPath('./td//text()')
title_cell = XPath('./td/*/a|./td/a')

for row in news_releases:
  cells = cell_text(row)
  date = cells[0]
  title = cells[1]
  category = cells[2]
  pr_link = title_cell(row)

  if not pr_link:
    print("No PR link for row %s" % tostring(row))
    continue

  href = pr_link[0].get('href')
  print("href=%s" % href)
  pr_els = fetch_press_release(href)
  if not pr_els:
    print("No PR content in %s" % href)
    continue

  pr = pr_els[0]
  debug_html_element(pr)

  # build JSON
  pr_data = parse_press_release(pr)
  pr_data['href'] = href
  pr_data['date'] = date
  pr_data['title'] = title
  pr_data['category'] = category

  json_file_name = hashlib.sha1(href.encode()).hexdigest() + '.json'
  with open('json/'+json_file_name, 'w') as json_file:
    json.dump(pr_data, json_file)

